import os
from app.utils.jina_embed import get_embedding  
from app.utils.qdrant_client import query_qdrant 
from app.utils.gemini import query_gemini 
#this is the rag_pipeline. When a query comes this code is hit.
def generate_response(user_query: str, top_k: int = 5) -> str:
    #query is converted into embedding
    query_embedding = get_embedding(user_query)
    if not query_embedding:
        return "Failed to embed the query."

    #embedding stored in qdrant
    documents = query_qdrant(query_embedding, top_k=top_k)
    if not documents:
        return "I couldn't find any relevant information."

    #context generated for LLM
    context = "\n\n".join(doc['text'] for doc in documents)

    #promt generated with context and user_query embeddings
    prompt = f"""Use the following context to answer the question:\n\n{context}\n\nQuestion: {user_query}"""
    #answer generated by LLM
    answer = query_gemini(prompt)
    return answer or "Gemini could not generate a response."
